{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eb857e-71ff-4e8c-8589-89af5b09b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Q1. What is the Filter method in feature selection, and how does it work?\n",
    "##The Filter method is a feature selection technique used in machine learning to select a subset of relevant features from a dataset. It works by evaluating each feature individually based on its correlation or relevance to the target variable, without considering the interactions between features.\n",
    "# 1 Feature Scoring:\n",
    "#2 Ranking Features:\n",
    "# 3 Selecting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42d3a5f2-e89b-4476-a53e-58eede42b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "##Filter methods are generally faster and more scalable, suitable for large datasets, but may miss out on feature interactions.\n",
    "##Wrapper methods are more thorough, considering feature interactions, but are computationally intensive and model-dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81a13a9-b33a-4fae-977a-37549c70a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Q3. What are some common techniques used in Embedded feature selection methods?\n",
    "\n",
    "##1. Regularization-based methods: Such as L1 and L2 regularization, which allow features to be set to zero to control model complexity.\n",
    "#2. Tree-based methods: Such as Random Forest and Gradient Boosting, which use tree structures to evaluate feature importance.\n",
    "#3. Neural Network-based methods: Such as Deep Learning models, which use neural network architectures to evaluate feature importance.\n",
    "#4. Sparse Modeling: Which allows features to be set to zero to control model complexity.\n",
    "#5. Feature Attribution Methods: Such as SHAP values, which divide the model's output to evaluate feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0ed439b-dfe4-4d77-b754-5fc8801a4c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Q4. What are some drawbacks of using the Filter method for feature selection?\n",
    "\n",
    "#1. Ignores feature interactions: Filter methods evaluate each feature individually, ignoring interactions between features.\n",
    "#2. Redundant features: May select multiple features with similar information, leading to redundancy.\n",
    "#3. Correlated features: May select highly correlated features, which can lead to multicollinearity.\n",
    "#4. No consideration of model performance: Filter methods do not consider the performance of the machine learning model.\n",
    "#5. Overemphasis on univariate relationships: May prioritize features with strong univariate relationships, even if they are not relevant in a multivariate context.\n",
    "#6. Sensitive to noise and outliers: Filter methods can be affected by noisy or outlier data points.\n",
    "#7. Difficult to determine the optimal number of features: Filter methods often require a predefined threshold or number of features to select.\n",
    "#8. Lack of interpretability: Filter methods may not provide clear insights into feature importance or relationships.\n",
    "#9. Not suitable for complex datasets: Filter methods may struggle with high-dimensional or complex datasets.\n",
    "#10. May not generalize well: Filter methods may not perform well on unseen data or in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d67fabd5-4845-4d20-9d84-6aea9f7c850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature salection ?\n",
    "\n",
    "#1. High-dimensional datasets: Filter methods are faster and more scalable for large datasets.\n",
    "#2. Computational resources are limited: Filter methods require less computational power and memory.\n",
    "#3. Simple feature relationships: Filter methods are suitable when feature relationships are straightforward and univariate.\n",
    "#4. Initial feature screening: Filter methods can be used for initial feature screening to reduce the feature space.\n",
    "#5. Data exploration: Filter methods can help understand feature distributions and correlations.\n",
    "#6. Interpretability is important: Filter methods provide clear feature importance scores and rankings.\n",
    "#7. Model-agnostic: Filter methods are independent of machine learning algorithms.\n",
    "#8. Fast prototyping: Filter methods enable quick feature selection and model testing.\n",
    "#9. Large number of features: Filter methods can handle a large number of features.\n",
    "#10. No need for hyperparameter tuning: Filter methods do not require hyperparameter tuning.\n",
    "\n",
    "#In contrast, Wrapper methods are preferred when:\n",
    "\n",
    "#- Feature interactions are complex\n",
    "#- Model performance is critical\n",
    "#- Hyperparameter tuning is necessary\n",
    "#- Interpretability is not essential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344b16eb-0597-44f4-8dec-62542c606a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.\n",
    "\n",
    "\n",
    "\n",
    "#To use the Embedded method for feature selection in predicting soccer match outcomes, I would follow these steps:\n",
    "\n",
    "#1. Split data: Divide the dataset into training and testing sets.\n",
    "\n",
    "#2. Train a model: Train a machine learning model (e.g., Random Forest, Gradient Boosting) on the training data with all features.\n",
    "\n",
    "#3. Permutation importance: Calculate the permutation importance of each feature using the trained model.\n",
    "\n",
    "#4. SHAP values: Calculate SHAP (SHapley Additive exPlanations) values for each feature to understand its contribution to the model's predictions.\n",
    "\n",
    "#5. Feature importance ranking: Rank features based on their permutation importance and SHAP values.\n",
    "\n",
    "#6. Select top features: Select the top-ranked features (e.g., top 10) as the most relevant for the model.\n",
    "\n",
    "#7. Refine the model: Retrain the model with the selected features and evaluate its performance on the testing data.\n",
    "\n",
    "#Some specific embedded methods I would use include:\n",
    "\n",
    "#- Permutation Feature Importance\n",
    "#- SHAP values\n",
    "#- LIME (Local Interpretable Model-agnostic Explanations)\n",
    "#- TreeExplainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd49cd2-e9fb-4ebe-923d-335123057364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
